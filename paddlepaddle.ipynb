{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paddlepaddle",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangkang90/machineLearningExercise/blob/master/paddlepaddle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ai0jeMZOrzs9",
        "colab_type": "code",
        "outputId": "d32ae038-4df1-474b-d667-be0c84450080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1141
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip install paddlepaddle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting paddlepaddle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/99/35eb655b0f3d6aa9eb4ba69179e69f13954699008fa3b664d3a19a9683da/paddlepaddle-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (49.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 49.7MB 927kB/s \n",
            "\u001b[?25hCollecting protobuf==3.1 (from paddlepaddle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/30/ab593c6ae73b45a5ef0b0af24908e8aec27f79efcda2e64a3df7af0b92a2/protobuf-3.1.0-py2.py3-none-any.whl (339kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 18.0MB/s \n",
            "\u001b[?25hCollecting recordio>=0.1.0 (from paddlepaddle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/30/14a03c04164c706a5b9c0bf5249a0599c87911c144cf78e9e9615abade43/recordio-0.1.7-cp36-cp36m-manylinux1_x86_64.whl (897kB)\n",
            "\u001b[K    100% |████████████████████████████████| 901kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from paddlepaddle) (3.4.5.20)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from paddlepaddle) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from paddlepaddle) (1.14.6)\n",
            "Collecting rarfile (from paddlepaddle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/a4/8b4abc72310da6fa53b6de8de1019e0516885d05369d6c91cba23476abe5/rarfile-3.0.tar.gz (110kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from paddlepaddle) (1.1.0)\n",
            "Collecting requests==2.9.2 (from paddlepaddle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/e7/229a428b8eb9a7f925ef16ff09ab25856efe789410d661f10157919f2ae2/requests-2.9.2-py2.py3-none-any.whl (502kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from paddlepaddle) (0.10.1)\n",
            "Collecting matplotlib==2.2.3 (from paddlepaddle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from paddlepaddle) (1.11.0)\n",
            "Requirement already satisfied: nltk>=3.2.2 in /usr/local/lib/python3.6/dist-packages (from paddlepaddle) (3.2.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf==3.1->paddlepaddle) (40.8.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->paddlepaddle) (0.46)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->paddlepaddle) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->paddlepaddle) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->paddlepaddle) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->paddlepaddle) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->paddlepaddle) (1.0.1)\n",
            "Building wheels for collected packages: rarfile\n",
            "  Building wheel for rarfile (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/dc/84/da/8aff50941f548db5384b076d5a6a6afea0cd12672e0326edc4\n",
            "Successfully built rarfile\n",
            "\u001b[31mtweepy 3.6.0 has requirement requests>=2.11.1, but you'll have requests 2.9.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mtfds-nightly 1.0.1.dev201902190105 has requirement protobuf>=3.6.1, but you'll have protobuf 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow 1.13.0rc2 has requirement protobuf>=3.6.1, but you'll have protobuf 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow-metadata 0.9.0 has requirement protobuf<4,>=3.6.0, but you'll have protobuf 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow-hub 0.2.0 has requirement protobuf>=3.4.0, but you'll have protobuf 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorboard 1.12.2 has requirement protobuf>=3.4.0, but you'll have protobuf 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement requests<3.0.0,>=2.13.0, but you'll have requests 2.9.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogleapis-common-protos 1.5.8 has requirement protobuf>=3.6.0, but you'll have protobuf 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 0.0.1a1 has requirement requests~=2.18.0, but you'll have requests 2.9.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-api-core 1.7.0 has requirement protobuf>=3.4.0, but you'll have protobuf 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-api-core 1.7.0 has requirement requests<3.0.0dev,>=2.18.0, but you'll have requests 2.9.2 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: protobuf, recordio, rarfile, requests, matplotlib, paddlepaddle\n",
            "  Found existing installation: protobuf 3.6.1\n",
            "    Uninstalling protobuf-3.6.1:\n",
            "      Successfully uninstalled protobuf-3.6.1\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "  Found existing installation: matplotlib 3.0.2\n",
            "    Uninstalling matplotlib-3.0.2:\n",
            "      Successfully uninstalled matplotlib-3.0.2\n",
            "Successfully installed matplotlib-2.2.3 paddlepaddle-1.2.1 protobuf-3.1.0 rarfile-3.0 recordio-0.1.7 requests-2.9.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "matplotlib",
                  "mpl_toolkits",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "C4-bACpzsfrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#   Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy\n",
        "import paddle\n",
        "import paddle.fluid as fluid\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "PASS_NUM = 5\n",
        "\n",
        "\n",
        "def loss_net(hidden, label):\n",
        "    prediction = fluid.layers.fc(input=hidden, size=10, act='softmax')\n",
        "    loss = fluid.layers.cross_entropy(input=prediction, label=label)\n",
        "    avg_loss = fluid.layers.mean(loss)\n",
        "    acc = fluid.layers.accuracy(input=prediction, label=label)\n",
        "    return prediction, avg_loss, acc\n",
        "\n",
        "\n",
        "def multilayer_perceptron(img, label):\n",
        "    img = fluid.layers.fc(input=img, size=200, act='tanh')\n",
        "    hidden = fluid.layers.fc(input=img, size=200, act='tanh')\n",
        "    return loss_net(hidden, label)\n",
        "\n",
        "\n",
        "def softmax_regression(img, label):\n",
        "    return loss_net(img, label)\n",
        "\n",
        "\n",
        "def convolutional_neural_network(img, label):\n",
        "    conv_pool_1 = fluid.nets.simple_img_conv_pool(\n",
        "        input=img,\n",
        "        filter_size=5,\n",
        "        num_filters=20,\n",
        "        pool_size=2,\n",
        "        pool_stride=2,\n",
        "        act=\"relu\")\n",
        "    conv_pool_1 = fluid.layers.batch_norm(conv_pool_1)\n",
        "    conv_pool_2 = fluid.nets.simple_img_conv_pool(\n",
        "        input=conv_pool_1,\n",
        "        filter_size=5,\n",
        "        num_filters=50,\n",
        "        pool_size=2,\n",
        "        pool_stride=2,\n",
        "        act=\"relu\")\n",
        "    return loss_net(conv_pool_2, label)\n",
        "\n",
        "\n",
        "def train(nn_type,\n",
        "          use_cuda,\n",
        "          save_dirname=None,\n",
        "          model_filename=None,\n",
        "          params_filename=None):\n",
        "    if use_cuda and not fluid.core.is_compiled_with_cuda():\n",
        "        return\n",
        "\n",
        "    img = fluid.layers.data(name='img', shape=[1, 28, 28], dtype='float32')\n",
        "    label = fluid.layers.data(name='label', shape=[1], dtype='int64')\n",
        "\n",
        "    if nn_type == 'softmax_regression':\n",
        "        net_conf = softmax_regression\n",
        "    elif nn_type == 'multilayer_perceptron':\n",
        "        net_conf = multilayer_perceptron\n",
        "    else:\n",
        "        net_conf = convolutional_neural_network\n",
        "\n",
        "    prediction, avg_loss, acc = net_conf(img, label)\n",
        "\n",
        "    test_program = fluid.default_main_program().clone(for_test=True)\n",
        "\n",
        "    optimizer = fluid.optimizer.Adam(learning_rate=0.001)\n",
        "    optimizer.minimize(avg_loss)\n",
        "\n",
        "    def train_test(train_test_program, train_test_feed, train_test_reader):\n",
        "        acc_set = []\n",
        "        avg_loss_set = []\n",
        "        for test_data in train_test_reader():\n",
        "            acc_np, avg_loss_np = exe.run(\n",
        "                program=train_test_program,\n",
        "                feed=train_test_feed.feed(test_data),\n",
        "                fetch_list=[acc, avg_loss])\n",
        "            acc_set.append(float(acc_np))\n",
        "            avg_loss_set.append(float(avg_loss_np))\n",
        "        # get test acc and loss\n",
        "        acc_val_mean = numpy.array(acc_set).mean()\n",
        "        avg_loss_val_mean = numpy.array(avg_loss_set).mean()\n",
        "        return avg_loss_val_mean, acc_val_mean\n",
        "\n",
        "    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
        "\n",
        "    exe = fluid.Executor(place)\n",
        "\n",
        "    train_reader = paddle.batch(\n",
        "        paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500),\n",
        "        batch_size=BATCH_SIZE)\n",
        "    test_reader = paddle.batch(\n",
        "        paddle.dataset.mnist.test(), batch_size=BATCH_SIZE)\n",
        "    feeder = fluid.DataFeeder(feed_list=[img, label], place=place)\n",
        "\n",
        "    exe.run(fluid.default_startup_program())\n",
        "    main_program = fluid.default_main_program()\n",
        "    epochs = [epoch_id for epoch_id in range(PASS_NUM)]\n",
        "\n",
        "    lists = []\n",
        "    step = 0\n",
        "    for epoch_id in epochs:\n",
        "        for step_id, data in enumerate(train_reader()):\n",
        "            metrics = exe.run(\n",
        "                main_program,\n",
        "                feed=feeder.feed(data),\n",
        "                fetch_list=[avg_loss, acc])\n",
        "            if step % 100 == 0:\n",
        "                print(\"Pass %d, Batch %d, Cost %f\" % (step, epoch_id,\n",
        "                                                      metrics[0]))\n",
        "            step += 1\n",
        "        # test for epoch\n",
        "        avg_loss_val, acc_val = train_test(\n",
        "            train_test_program=test_program,\n",
        "            train_test_reader=test_reader,\n",
        "            train_test_feed=feeder)\n",
        "\n",
        "        print(\"Test with Epoch %d, avg_cost: %s, acc: %s\" %\n",
        "              (epoch_id, avg_loss_val, acc_val))\n",
        "        lists.append((epoch_id, avg_loss_val, acc_val))\n",
        "        if save_dirname is not None:\n",
        "            fluid.io.save_inference_model(\n",
        "                save_dirname, [\"img\"], [prediction],\n",
        "                exe,\n",
        "                model_filename=model_filename,\n",
        "                params_filename=params_filename)\n",
        "\n",
        "    # find the best pass\n",
        "    best = sorted(lists, key=lambda list: float(list[1]))[0]\n",
        "    print('Best pass is %s, testing Avgcost is %s' % (best[0], best[1]))\n",
        "    print('The classification accuracy is %.2f%%' % (float(best[2]) * 100))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CbXFNNHgwodz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def infer(use_cuda,\n",
        "          save_dirname=None,\n",
        "          model_filename=None,\n",
        "          params_filename=None):\n",
        "    if save_dirname is None:\n",
        "        return\n",
        "\n",
        "    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
        "    exe = fluid.Executor(place)\n",
        "\n",
        "    def load_image(file):\n",
        "        im = Image.open(file).convert('L')\n",
        "        im = im.resize((28, 28), Image.ANTIALIAS)\n",
        "        im = numpy.array(im).reshape(1, 1, 28, 28).astype(numpy.float32)\n",
        "        im = im / 255.0 * 2.0 - 1.0\n",
        "        return im\n",
        "\n",
        "  #  cur_dir = os.path.dirname(os.path.realpath(__file__))\n",
        "  #  tensor_img = load_image(cur_dir + '/image/infer_3.png')\n",
        "    tensor_img = load_image('/content/gdrive/My Drive/image/photo1.png')\n",
        "\n",
        "    inference_scope = fluid.core.Scope()\n",
        "    with fluid.scope_guard(inference_scope):\n",
        "        # Use fluid.io.load_inference_model to obtain the inference program desc,\n",
        "        # the feed_target_names (the names of variables that will be feeded\n",
        "        # data using feed operators), and the fetch_targets (variables that\n",
        "        # we want to obtain data from using fetch operators).\n",
        "        [inference_program, feed_target_names,\n",
        "         fetch_targets] = fluid.io.load_inference_model(\n",
        "             save_dirname, exe, model_filename, params_filename)\n",
        "\n",
        "        # Construct feed as a dictionary of {feed_target_name: feed_target_data}\n",
        "        # and results will contain a list of data corresponding to fetch_targets.\n",
        "        results = exe.run(\n",
        "            inference_program,\n",
        "            feed={feed_target_names[0]: tensor_img},\n",
        "            fetch_list=fetch_targets)\n",
        "        lab = numpy.argsort(results)\n",
        "        print(\"Inference result of image/infer_3.png is: %d\" % lab[0][0][-1])\n",
        "\n",
        "\n",
        "def main(use_cuda, nn_type):\n",
        "    model_filename = None\n",
        "    params_filename = None\n",
        "    save_dirname = \"recognize_digits_\" + nn_type + \".inference.model\"\n",
        "\n",
        "    # call train() with is_local argument to run distributed train\n",
        "    train(\n",
        "        nn_type=nn_type,\n",
        "        use_cuda=use_cuda,\n",
        "        save_dirname=save_dirname,\n",
        "        model_filename=model_filename,\n",
        "        params_filename=params_filename)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5B7wfSk6IG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    use_cuda = False\n",
        "    # predict = 'softmax_regression' # uncomment for Softmax\n",
        "    # predict = 'multilayer_perceptron' # uncomment for MLP\n",
        "    predict = 'convolutional_neural_network'  # uncomment for LeNet5\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJllj6v-6Ora",
        "colab_type": "code",
        "outputId": "04916a79-5f1c-42d2-bcd4-c2d165afaff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1627
        }
      },
      "cell_type": "code",
      "source": [
        "   main(use_cuda=use_cuda, nn_type=predict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================]\n",
            "[==================================================]\n",
            "[==================================================]\n",
            "[==================================================]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pass 0, Batch 0, Cost 3.483964\n",
            "Pass 100, Batch 0, Cost 0.215237\n",
            "Pass 200, Batch 0, Cost 0.258138\n",
            "Pass 300, Batch 0, Cost 0.206161\n",
            "Pass 400, Batch 0, Cost 0.155898\n",
            "Pass 500, Batch 0, Cost 0.116960\n",
            "Pass 600, Batch 0, Cost 0.076878\n",
            "Pass 700, Batch 0, Cost 0.064189\n",
            "Pass 800, Batch 0, Cost 0.031875\n",
            "Pass 900, Batch 0, Cost 0.044066\n",
            "Test with Epoch 0, avg_cost: 0.09022811680897228, acc: 0.9709394904458599\n",
            "Pass 1000, Batch 1, Cost 0.031239\n",
            "Pass 1100, Batch 1, Cost 0.039900\n",
            "Pass 1200, Batch 1, Cost 0.046206\n",
            "Pass 1300, Batch 1, Cost 0.029625\n",
            "Pass 1400, Batch 1, Cost 0.048268\n",
            "Pass 1500, Batch 1, Cost 0.040052\n",
            "Pass 1600, Batch 1, Cost 0.016050\n",
            "Pass 1700, Batch 1, Cost 0.007789\n",
            "Pass 1800, Batch 1, Cost 0.024415\n",
            "Test with Epoch 1, avg_cost: 0.06500062548484435, acc: 0.9793988853503185\n",
            "Pass 1900, Batch 2, Cost 0.007077\n",
            "Pass 2000, Batch 2, Cost 0.111271\n",
            "Pass 2100, Batch 2, Cost 0.078238\n",
            "Pass 2200, Batch 2, Cost 0.005146\n",
            "Pass 2300, Batch 2, Cost 0.021095\n",
            "Pass 2400, Batch 2, Cost 0.012375\n",
            "Pass 2500, Batch 2, Cost 0.019071\n",
            "Pass 2600, Batch 2, Cost 0.050207\n",
            "Pass 2700, Batch 2, Cost 0.031193\n",
            "Pass 2800, Batch 2, Cost 0.006543\n",
            "Test with Epoch 2, avg_cost: 0.04515828603236433, acc: 0.9857683121019108\n",
            "Pass 2900, Batch 3, Cost 0.054749\n",
            "Pass 3000, Batch 3, Cost 0.107552\n",
            "Pass 3100, Batch 3, Cost 0.041076\n",
            "Pass 3200, Batch 3, Cost 0.019042\n",
            "Pass 3300, Batch 3, Cost 0.025746\n",
            "Pass 3400, Batch 3, Cost 0.007245\n",
            "Pass 3500, Batch 3, Cost 0.019489\n",
            "Pass 3600, Batch 3, Cost 0.003011\n",
            "Pass 3700, Batch 3, Cost 0.023458\n",
            "Test with Epoch 3, avg_cost: 0.0510598875991931, acc: 0.9847730891719745\n",
            "Pass 3800, Batch 4, Cost 0.004234\n",
            "Pass 3900, Batch 4, Cost 0.005758\n",
            "Pass 4000, Batch 4, Cost 0.003660\n",
            "Pass 4100, Batch 4, Cost 0.005270\n",
            "Pass 4200, Batch 4, Cost 0.101978\n",
            "Pass 4300, Batch 4, Cost 0.071164\n",
            "Pass 4400, Batch 4, Cost 0.039892\n",
            "Pass 4500, Batch 4, Cost 0.001587\n",
            "Pass 4600, Batch 4, Cost 0.013103\n",
            "Test with Epoch 4, avg_cost: 0.04641706325727752, acc: 0.9875597133757962\n",
            "Best pass is 2, testing Avgcost is 0.04515828603236433\n",
            "The classification accuracy is 98.58%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d98f36c66645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-5e079e8cd3b9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(use_cuda, nn_type)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msave_dirname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dirname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmodel_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         params_filename=params_filename)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-5e079e8cd3b9>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(use_cuda, save_dirname, model_filename, params_filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m#  cur_dir = os.path.dirname(os.path.realpath(__file__))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m#  tensor_img = load_image(cur_dir + '/image/infer_3.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtensor_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/image/photo1.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minference_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfluid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-5e079e8cd3b9>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2312\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/image/photo1.png'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0r52nx2H4iPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}